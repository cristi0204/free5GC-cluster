
autodiscover:

  enabled: true

  spec:
    type: filebeat
    version: 8.4.3
    elasticsearchRef:
      name: elastic-eck-elasticsearch
    #kibanaRef:
       #name: kibana
    config:
      filebeat.autodiscover.providers:
      - node: ${NODE_NAME}
        type: kubernetes
        hints.default_config.enabled: "false"
        templates:
        - condition.equals.kubernetes.namespace: grafana
          config:
          - paths: ["/var/log/containers/*${data.kubernetes.container.id}.log"]
            type: container
        - condition.equals.kubernetes.namespace: kafka
          config:
          - paths: ["/var/log/containers/*${data.kubernetes.container.id}.log"]
            type: container
        - condition.equals.kubernetes.namespace: elastic-system
          config:
          - paths: ["/var/log/containers/*${data.kubernetes.container.id}.log"]
            type: container
        - condition.equals.kubernetes.namespace: victoriametrics
          config:
          - paths: ["/var/log/containers/*${data.kubernetes.container.id}.log"]
            type: container
        - condition.equals.kubernetes.namespace: victoriametrics-workload
          config:
          - paths: ["/var/log/containers/*${data.kubernetes.container.id}.log"]
            type: container
      # - condition.equals.kubernetes.labels.log-label: "true"
      #   config:
      #     - paths: ["/var/log/containers/*${data.kubernetes.container.id}.log"]
      #       type: container
      processors:
      - add_cloud_metadata: {}
      - add_host_metadata: {}
    daemonSet:
      podTemplate:
        spec:
          serviceAccountName: filebeat
          automountServiceAccountToken: true
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirstWithHostNet
          hostNetwork: true # Allows to provide richer host metadata
          containers:
          - name: filebeat
            securityContext:
              runAsUser: 0
              # If using Red Hat OpenShift uncomment this:
              #privileged: true
            volumeMounts:
            - name: varlogcontainers
              mountPath: /var/log/containers
            - name: varlogpods
              mountPath: /var/log/pods
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
            env:
              - name: NODE_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
          volumes:
          - name: varlogcontainers
            hostPath:
              path: /var/log/containers
          - name: varlogpods
            hostPath:
              path: /var/log/pods
          - name: varlibdockercontainers
            hostPath:
              path: /var/lib/docker/containers



configmap:

  enabled: true

  filebeatYaml: |-
                  filebeat.inputs:
                  - type: container
                    paths:
                      - /var/log/containers/
                    processors:
                      - add_kubernetes_metadata:
                          host: ${NODE_NAME}
                          matchers:
                          - logs_path:
                              logs_path: "/var/log/containers/"
                  setup.dashboards.enabled: true
                  setup.kibana.host: elastic-eck-kibana-kb-http:5601
                  setup.kibana.protocol: https
                  setup.kibana.ssl.enabled: true
                  setup.kibana.ssl.verification_mode: none
                  setup.kibana.username: elastic
                  setup.kibana.password: Y1luy7f0L0hTWcx4R56Cp377

                  # To enable hints based autodiscover, remove `filebeat.inputs` configuration and uncomment this:
                  #filebeat.autodiscover:
                  #  providers:
                  #    - type: kubernetes
                  #      node: ${NODE_NAME}
                  #      hints.enabled: true
                  #      hints.default_config:
                  #        type: container
                  #        paths:
                  #          - /var/log/containers/*${data.kubernetes.container.id}.log

                  processors:
                    - add_cloud_metadata:
                    - add_host_metadata:

                  cloud.id: ${ELASTIC_CLOUD_ID}
                  cloud.auth: ${ELASTIC_CLOUD_AUTH}

                  #output.elasticsearch:
                  #  hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}']
                  #  protocol: https
                  #  ssl.verification_mode: "none"      
                  #  username: ${ELASTICSEARCH_USERNAME}
                  #  password: ${ELASTICSEARCH_PASSWORD}

                  output.kafka:
                    # initial brokers for reading cluster metadata
                    #hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]
                    hostd: [${KAFKA_HOST}]

                    # message topic selection + partitioning
                    #this configuration uses a custom field, fields.log_topic, to set the topic for each event
                    #topic: '%{[fields.log_topic]}'
                    topic: ${KAFKA_TOPIC}
                    partition.round_robin:
                      reachable_only: false

                    required_acks: 1
                    compression: none
                    max_message_bytes: 1000000




daemonset:
#For the Daemonset the Configmap has to be enabled also
  enabled: true

  spec:
    selector:
      matchLabels:
        k8s-app: filebeat
    template:
      metadata:
        labels:
          k8s-app: filebeat
      spec:
        serviceAccountName: filebeat
        terminationGracePeriodSeconds: 30
        hostNetwork: true
        dnsPolicy: ClusterFirstWithHostNet
        containers:
        - name: filebeat
          image: docker.elastic.co/beats/filebeat:8.4.3
          args: [
            "-c", "/etc/filebeat.yml",
            "-e",
          ]
          env:
          - name: KAFKA_HOST
            value: kafka-cluster-kafka-bootstrap-external-elastic:9092
          - name: KAFKA_TOPIC
            value: logs
          - name: ELASTICSEARCH_HOST
            value: elastic-eck-elasticsearch-es-http
          - name: ELASTICSEARCH_PORT
            value: "9200"
          - name: ELASTICSEARCH_USERNAME
            value: elastic
          - name: ELASTICSEARCH_PASSWORD
            value: Y1luy7f0L0hTWcx4R56Cp377
          - name: ELASTIC_CLOUD_ID
            value:
          - name: ELASTIC_CLOUD_AUTH
            value:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          securityContext:
            runAsUser: 0
            # If using Red Hat OpenShift uncomment this:
            #privileged: true
          resources:
            limits:
              memory: 400Mi
            requests:
              cpu: 100m
              memory: 200Mi
          volumeMounts:
          - name: config
            mountPath: /etc/filebeat.yml
            readOnly: true
            subPath: filebeat.yml
          - name: data
            mountPath: /usr/share/filebeat/data
          - name: varlibdockercontainers
            mountPath: /var/lib/docker/containers
            readOnly: true
          - name: varlog
            mountPath: /var/log
            readOnly: true
        volumes:
        - name: config
          configMap:
            defaultMode: 0640
            name: filebeat-config
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: varlog
          hostPath:
            path: /var/log
        # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart
        - name: data
          hostPath:
            # When filebeat runs as non-root user, this directory needs to be writable by group (g+w).
            path: /var/lib/filebeat-data
            type: DirectoryOrCreate