Release "kafka" does not exist. Installing it now.
NAME: kafka
LAST DEPLOYED: Thu Sep 22 13:45:20 2022
NAMESPACE: kafka
STATUS: deployed
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
advertisedListeners: []
affinity: {}
allowEveryoneIfNoAclFound: true
allowPlaintextListener: true
args: []
auth:
  clientProtocol: plaintext
  externalClientProtocol: ""
  interBrokerProtocol: plaintext
  sasl:
    interBrokerMechanism: plain
    jaas:
      clientPasswords: []
      clientUsers:
      - user
      existingSecret: ""
      interBrokerPassword: ""
      interBrokerUser: admin
      zookeeperPassword: ""
      zookeeperUser: ""
    mechanisms: plain,scram-sha-256,scram-sha-512
  tls:
    autoGenerated: false
    endpointIdentificationAlgorithm: https
    existingSecret: ""
    existingSecrets: []
    jksKeystoreSAN: ""
    jksTruststore: ""
    jksTruststoreSecret: ""
    password: ""
    pemChainIncluded: false
    type: jks
  zookeeper:
    tls:
      enabled: false
      existingSecret: ""
      existingSecretKeystoreKey: zookeeper.keystore.jks
      existingSecretTruststoreKey: zookeeper.truststore.jks
      passwordsSecret: ""
      passwordsSecretKeystoreKey: keystore-password
      passwordsSecretTruststoreKey: truststore-password
      type: jks
      verifyHostname: true
authorizerClassName: ""
autoCreateTopicsEnable: false
clusterDomain: cluster.local
command:
- /scripts/setup.sh
commonAnnotations: {}
commonLabels: {}
config: ""
containerPorts:
  client: 9092
  external: 9094
  internal: 9093
containerSecurityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1001
customLivenessProbe: {}
customReadinessProbe: {}
customStartupProbe: {}
defaultReplicationFactor: 2
deleteTopicEnable: true
diagnosticMode:
  args:
  - infinity
  command:
  - sleep
  enabled: false
existingConfigmap: ""
existingLog4jConfigMap: ""
externalAccess:
  autoDiscovery:
    enabled: false
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/kubectl
      tag: 1.24.4-debian-11-r3
    resources:
      limits: {}
      requests: {}
  enabled: false
  service:
    annotations: {}
    domain: ""
    extraPorts: []
    labels: {}
    loadBalancerAnnotations: []
    loadBalancerIPs: []
    loadBalancerNames: []
    loadBalancerSourceRanges: []
    nodePorts: []
    ports:
      external: 9094
    type: LoadBalancer
    useHostIPs: false
    usePodIPs: false
externalZookeeper:
  servers: []
extraDeploy: []
extraEnvVars: []
extraEnvVarsCM: ""
extraEnvVarsSecret: ""
extraVolumeMounts: []
extraVolumes: []
fullnameOverride: ""
global:
  imagePullSecrets: []
  imageRegistry: ""
  storageClass: ""
heapOpts: -Xmx1024m -Xms1024m
hostAliases: []
hostIPC: false
hostNetwork: false
image:
  debug: false
  digest: ""
  pullPolicy: IfNotPresent
  pullSecrets: []
  registry: docker.io
  repository: bitnami/kafka
  tag: 3.2.1-debian-11-r9
initContainers: []
interBrokerListenerName: INTERNAL
kubeVersion: ""
lifecycleHooks: {}
listenerSecurityProtocolMap: ""
listeners: []
livenessProbe:
  enabled: true
  failureThreshold: 3
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
log4j: ""
logFlushIntervalMessages: _10000
logFlushIntervalMs: 1000
logPersistence:
  accessModes:
  - ReadWriteOnce
  annotations: {}
  enabled: false
  existingClaim: ""
  mountPath: /opt/bitnami/kafka/logs
  selector: {}
  size: 8Gi
  storageClass: ""
logRetentionBytes: _1073741824
logRetentionCheckIntervalMs: 300000
logRetentionHours: 168
logSegmentBytes: _1073741824
logsDirs: /bitnami/kafka/data
maxMessageBytes: _1000012
metrics:
  jmx:
    config: |-
      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      ssl: false
      {{- if .Values.metrics.jmx.whitelistObjectNames }}
      whitelistObjectNames: ["{{ join "\",\"" .Values.metrics.jmx.whitelistObjectNames }}"]
      {{- end }}
    containerPorts:
      metrics: 5556
    containerSecurityContext:
      enabled: true
      runAsNonRoot: true
      runAsUser: 1001
    enabled: false
    existingConfigmap: ""
    extraRules: ""
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/jmx-exporter
      tag: 0.17.0-debian-11-r30
    resources:
      limits: {}
      requests: {}
    service:
      annotations:
        prometheus.io/path: /
        prometheus.io/port: '{{ .Values.metrics.jmx.service.ports.metrics }}'
        prometheus.io/scrape: "true"
      clusterIP: ""
      ports:
        metrics: 5556
      sessionAffinity: None
    whitelistObjectNames:
    - kafka.controller:*
    - kafka.server:*
    - java.lang:*
    - kafka.network:*
    - kafka.log:*
  kafka:
    affinity: {}
    args: []
    certificatesSecret: ""
    command: []
    containerPorts:
      metrics: 9308
    containerSecurityContext:
      enabled: true
      runAsNonRoot: true
      runAsUser: 1001
    enabled: false
    extraFlags: {}
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/kafka-exporter
      tag: 1.6.0-debian-11-r1
    initContainers: []
    nodeAffinityPreset:
      key: ""
      type: ""
      values: []
    nodeSelector: {}
    podAffinityPreset: ""
    podAnnotations: {}
    podAntiAffinityPreset: soft
    podLabels: {}
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    priorityClassName: ""
    resources:
      limits: {}
      requests: {}
    schedulerName: ""
    service:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '{{ .Values.metrics.kafka.service.ports.metrics }}'
        prometheus.io/scrape: "true"
      clusterIP: ""
      ports:
        metrics: 9308
      sessionAffinity: None
    serviceAccount:
      automountServiceAccountToken: true
      create: true
      name: ""
    sidecars: []
    tlsCaCert: ca-file
    tlsCaSecret: ""
    tlsCert: cert-file
    tlsKey: key-file
    tolerations: []
    topologySpreadConstraints: []
  serviceMonitor:
    enabled: false
    honorLabels: false
    interval: ""
    jobLabel: ""
    labels: {}
    metricRelabelings: []
    namespace: ""
    relabelings: []
    scrapeTimeout: ""
    selector: {}
minBrokerId: 0
nameOverride: ""
networkPolicy:
  allowExternal: true
  egressRules:
    customRules: []
  enabled: false
  explicitNamespacesSelector: {}
  externalAccess:
    from: []
nodeAffinityPreset:
  key: ""
  type: ""
  values: []
nodeSelector: {}
numIoThreads: 8
numNetworkThreads: 3
numPartitions: 5
numRecoveryThreadsPerDataDir: 1
offsetsTopicReplicationFactor: 1
pdb:
  create: false
  maxUnavailable: 1
  minAvailable: ""
persistence:
  accessModes:
  - ReadWriteOnce
  annotations: {}
  enabled: true
  existingClaim: ""
  mountPath: /bitnami/kafka
  selector: {}
  size: 8Gi
  storageClass: ""
podAffinityPreset: ""
podAnnotations: {}
podAntiAffinityPreset: soft
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext:
  enabled: true
  fsGroup: 1001
priorityClassName: ""
provisioning:
  args: []
  auth:
    tls:
      caCert: ca.crt
      cert: tls.crt
      certificatesSecret: ""
      key: tls.key
      keyPassword: ""
      keyPasswordSecretKey: key-password
      keystore: keystore.jks
      keystorePassword: ""
      keystorePasswordSecretKey: keystore-password
      passwordsSecret: ""
      truststore: truststore.jks
      truststorePassword: ""
      truststorePasswordSecretKey: truststore-password
      type: jks
  command: []
  containerSecurityContext:
    enabled: true
    runAsNonRoot: true
    runAsUser: 1001
  enabled: true
  extraEnvVars: []
  extraEnvVarsCM: ""
  extraEnvVarsSecret: ""
  extraProvisioningCommands: []
  extraVolumeMounts: []
  extraVolumes: []
  initContainers: []
  numPartitions: 5
  parallel: 1
  podAnnotations: {}
  podLabels: {}
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  postScript: ""
  preScript: ""
  replicationFactor: 2
  resources:
    limits: {}
    requests: {}
  schedulerName: ""
  sidecars: []
  tolerations: []
  topics:
  - config:
      flush.messages: 1
      max.message.bytes: 86000000
    name: logs
    partitions: 5
    replicationFactor: 2
  - config:
      flush.messages: 1
      max.message.bytes: 86000000
    name: metrics
    partitions: 5
    replicationFactor: 2
  waitForKafka: true
rbac:
  create: false
readinessProbe:
  enabled: true
  failureThreshold: 6
  initialDelaySeconds: 5
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
replicaCount: 3
resources:
  limits: {}
  requests: {}
schedulerName: ""
service:
  annotations: {}
  clusterIP: ""
  externalTrafficPolicy: Cluster
  extraPorts: []
  headless:
    annotations: {}
    labels: {}
  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  nodePorts:
    client: ""
    external: ""
  ports:
    client: 9092
    external: 9094
    internal: 9093
  sessionAffinity: None
  sessionAffinityConfig: {}
  type: ClusterIP
serviceAccount:
  annotations: {}
  automountServiceAccountToken: true
  create: true
  name: ""
sidecars: []
socketReceiveBufferBytes: 102400
socketRequestMaxBytes: _104857600
socketSendBufferBytes: 102400
startupProbe:
  enabled: false
  failureThreshold: 15
  initialDelaySeconds: 30
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1
superUsers: User:admin
terminationGracePeriodSeconds: ""
tolerations: []
topologySpreadConstraints: []
transactionStateLogMinIsr: 1
transactionStateLogReplicationFactor: 1
updateStrategy:
  rollingUpdate: {}
  type: RollingUpdate
volumePermissions:
  containerSecurityContext:
    runAsUser: 0
  enabled: false
  image:
    digest: ""
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 11-debian-11-r30
  resources:
    limits: {}
    requests: {}
zookeeper:
  auth:
    client:
      clientPassword: ""
      clientUser: ""
      enabled: false
      serverPasswords: ""
      serverUsers: ""
  enabled: true
  persistence:
    accessModes:
    - ReadWriteOnce
    enabled: true
    size: 8Gi
    storageClass: ""
  replicaCount: 3
zookeeperChrootPath: ""
zookeeperConnectionTimeoutMs: 6000

COMPUTED VALUES:
advertisedListeners: []
affinity: {}
allowEveryoneIfNoAclFound: true
allowPlaintextListener: true
args: []
auth:
  clientProtocol: plaintext
  externalClientProtocol: ""
  interBrokerProtocol: plaintext
  sasl:
    interBrokerMechanism: plain
    jaas:
      clientPasswords: []
      clientUsers:
      - user
      existingSecret: ""
      interBrokerPassword: ""
      interBrokerUser: admin
      zookeeperPassword: ""
      zookeeperUser: ""
    mechanisms: plain,scram-sha-256,scram-sha-512
  tls:
    autoGenerated: false
    endpointIdentificationAlgorithm: https
    existingSecret: ""
    existingSecrets: []
    jksKeystoreSAN: ""
    jksTruststore: ""
    jksTruststoreSecret: ""
    password: ""
    pemChainIncluded: false
    type: jks
  zookeeper:
    tls:
      enabled: false
      existingSecret: ""
      existingSecretKeystoreKey: zookeeper.keystore.jks
      existingSecretTruststoreKey: zookeeper.truststore.jks
      passwordsSecret: ""
      passwordsSecretKeystoreKey: keystore-password
      passwordsSecretTruststoreKey: truststore-password
      type: jks
      verifyHostname: true
authorizerClassName: ""
autoCreateTopicsEnable: false
clusterDomain: cluster.local
command:
- /scripts/setup.sh
common:
  exampleValue: common-chart
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
commonAnnotations: {}
commonLabels: {}
config: ""
containerPorts:
  client: 9092
  external: 9094
  internal: 9093
containerSecurityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1001
customLivenessProbe: {}
customReadinessProbe: {}
customStartupProbe: {}
defaultReplicationFactor: 2
deleteTopicEnable: true
diagnosticMode:
  args:
  - infinity
  command:
  - sleep
  enabled: false
existingConfigmap: ""
existingLog4jConfigMap: ""
externalAccess:
  autoDiscovery:
    enabled: false
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/kubectl
      tag: 1.24.4-debian-11-r3
    resources:
      limits: {}
      requests: {}
  enabled: false
  service:
    annotations: {}
    domain: ""
    extraPorts: []
    labels: {}
    loadBalancerAnnotations: []
    loadBalancerIPs: []
    loadBalancerNames: []
    loadBalancerSourceRanges: []
    nodePorts: []
    ports:
      external: 9094
    type: LoadBalancer
    useHostIPs: false
    usePodIPs: false
externalZookeeper:
  servers: []
extraDeploy: []
extraEnvVars: []
extraEnvVarsCM: ""
extraEnvVarsSecret: ""
extraVolumeMounts: []
extraVolumes: []
fullnameOverride: ""
global:
  imagePullSecrets: []
  imageRegistry: ""
  storageClass: ""
heapOpts: -Xmx1024m -Xms1024m
hostAliases: []
hostIPC: false
hostNetwork: false
image:
  debug: false
  digest: ""
  pullPolicy: IfNotPresent
  pullSecrets: []
  registry: docker.io
  repository: bitnami/kafka
  tag: 3.2.1-debian-11-r9
initContainers: []
interBrokerListenerName: INTERNAL
kubeVersion: ""
lifecycleHooks: {}
listenerSecurityProtocolMap: ""
listeners: []
livenessProbe:
  enabled: true
  failureThreshold: 3
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
log4j: ""
logFlushIntervalMessages: _10000
logFlushIntervalMs: 1000
logPersistence:
  accessModes:
  - ReadWriteOnce
  annotations: {}
  enabled: false
  existingClaim: ""
  mountPath: /opt/bitnami/kafka/logs
  selector: {}
  size: 8Gi
  storageClass: ""
logRetentionBytes: _1073741824
logRetentionCheckIntervalMs: 300000
logRetentionHours: 168
logSegmentBytes: _1073741824
logsDirs: /bitnami/kafka/data
maxMessageBytes: _1000012
metrics:
  jmx:
    config: |-
      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      ssl: false
      {{- if .Values.metrics.jmx.whitelistObjectNames }}
      whitelistObjectNames: ["{{ join "\",\"" .Values.metrics.jmx.whitelistObjectNames }}"]
      {{- end }}
    containerPorts:
      metrics: 5556
    containerSecurityContext:
      enabled: true
      runAsNonRoot: true
      runAsUser: 1001
    enabled: false
    existingConfigmap: ""
    extraRules: ""
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/jmx-exporter
      tag: 0.17.0-debian-11-r30
    resources:
      limits: {}
      requests: {}
    service:
      annotations:
        prometheus.io/path: /
        prometheus.io/port: '{{ .Values.metrics.jmx.service.ports.metrics }}'
        prometheus.io/scrape: "true"
      clusterIP: ""
      ports:
        metrics: 5556
      sessionAffinity: None
    whitelistObjectNames:
    - kafka.controller:*
    - kafka.server:*
    - java.lang:*
    - kafka.network:*
    - kafka.log:*
  kafka:
    affinity: {}
    args: []
    certificatesSecret: ""
    command: []
    containerPorts:
      metrics: 9308
    containerSecurityContext:
      enabled: true
      runAsNonRoot: true
      runAsUser: 1001
    enabled: false
    extraFlags: {}
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/kafka-exporter
      tag: 1.6.0-debian-11-r1
    initContainers: []
    nodeAffinityPreset:
      key: ""
      type: ""
      values: []
    nodeSelector: {}
    podAffinityPreset: ""
    podAnnotations: {}
    podAntiAffinityPreset: soft
    podLabels: {}
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    priorityClassName: ""
    resources:
      limits: {}
      requests: {}
    schedulerName: ""
    service:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '{{ .Values.metrics.kafka.service.ports.metrics }}'
        prometheus.io/scrape: "true"
      clusterIP: ""
      ports:
        metrics: 9308
      sessionAffinity: None
    serviceAccount:
      automountServiceAccountToken: true
      create: true
      name: ""
    sidecars: []
    tlsCaCert: ca-file
    tlsCaSecret: ""
    tlsCert: cert-file
    tlsKey: key-file
    tolerations: []
    topologySpreadConstraints: []
  serviceMonitor:
    enabled: false
    honorLabels: false
    interval: ""
    jobLabel: ""
    labels: {}
    metricRelabelings: []
    namespace: ""
    relabelings: []
    scrapeTimeout: ""
    selector: {}
minBrokerId: 0
nameOverride: ""
networkPolicy:
  allowExternal: true
  egressRules:
    customRules: []
  enabled: false
  explicitNamespacesSelector: {}
  externalAccess:
    from: []
nodeAffinityPreset:
  key: ""
  type: ""
  values: []
nodeSelector: {}
numIoThreads: 8
numNetworkThreads: 3
numPartitions: 5
numRecoveryThreadsPerDataDir: 1
offsetsTopicReplicationFactor: 1
pdb:
  create: false
  maxUnavailable: 1
  minAvailable: ""
persistence:
  accessModes:
  - ReadWriteOnce
  annotations: {}
  enabled: true
  existingClaim: ""
  mountPath: /bitnami/kafka
  selector: {}
  size: 8Gi
  storageClass: ""
podAffinityPreset: ""
podAnnotations: {}
podAntiAffinityPreset: soft
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext:
  enabled: true
  fsGroup: 1001
priorityClassName: ""
provisioning:
  args: []
  auth:
    tls:
      caCert: ca.crt
      cert: tls.crt
      certificatesSecret: ""
      key: tls.key
      keyPassword: ""
      keyPasswordSecretKey: key-password
      keystore: keystore.jks
      keystorePassword: ""
      keystorePasswordSecretKey: keystore-password
      passwordsSecret: ""
      truststore: truststore.jks
      truststorePassword: ""
      truststorePasswordSecretKey: truststore-password
      type: jks
  command: []
  containerSecurityContext:
    enabled: true
    runAsNonRoot: true
    runAsUser: 1001
  enabled: true
  extraEnvVars: []
  extraEnvVarsCM: ""
  extraEnvVarsSecret: ""
  extraProvisioningCommands: []
  extraVolumeMounts: []
  extraVolumes: []
  initContainers: []
  numPartitions: 5
  parallel: 1
  podAnnotations: {}
  podLabels: {}
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  postScript: ""
  preScript: ""
  replicationFactor: 2
  resources:
    limits: {}
    requests: {}
  schedulerName: ""
  sidecars: []
  tolerations: []
  topics:
  - config:
      flush.messages: 1
      max.message.bytes: 86000000
    name: logs
    partitions: 5
    replicationFactor: 2
  - config:
      flush.messages: 1
      max.message.bytes: 86000000
    name: metrics
    partitions: 5
    replicationFactor: 2
  waitForKafka: true
rbac:
  create: false
readinessProbe:
  enabled: true
  failureThreshold: 6
  initialDelaySeconds: 5
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
replicaCount: 3
resources:
  limits: {}
  requests: {}
schedulerName: ""
service:
  annotations: {}
  clusterIP: ""
  externalTrafficPolicy: Cluster
  extraPorts: []
  headless:
    annotations: {}
    labels: {}
  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  nodePorts:
    client: ""
    external: ""
  ports:
    client: 9092
    external: 9094
    internal: 9093
  sessionAffinity: None
  sessionAffinityConfig: {}
  type: ClusterIP
serviceAccount:
  annotations: {}
  automountServiceAccountToken: true
  create: true
  name: ""
sidecars: []
socketReceiveBufferBytes: 102400
socketRequestMaxBytes: _104857600
socketSendBufferBytes: 102400
startupProbe:
  enabled: false
  failureThreshold: 15
  initialDelaySeconds: 30
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1
superUsers: User:admin
terminationGracePeriodSeconds: ""
tolerations: []
topologySpreadConstraints: []
transactionStateLogMinIsr: 1
transactionStateLogReplicationFactor: 1
updateStrategy:
  rollingUpdate: {}
  type: RollingUpdate
volumePermissions:
  containerSecurityContext:
    runAsUser: 0
  enabled: false
  image:
    digest: ""
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 11-debian-11-r30
  resources:
    limits: {}
    requests: {}
zookeeper:
  affinity: {}
  args: []
  auth:
    client:
      clientPassword: ""
      clientUser: ""
      enabled: false
      existingSecret: ""
      serverPasswords: ""
      serverUsers: ""
    quorum:
      enabled: false
      existingSecret: ""
      learnerPassword: ""
      learnerUser: ""
      serverPasswords: ""
      serverUsers: ""
  autopurge:
    purgeInterval: 0
    snapRetainCount: 3
  clusterDomain: cluster.local
  command:
  - /scripts/setup.sh
  common:
    exampleValue: common-chart
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
  commonAnnotations: {}
  commonLabels: {}
  configuration: ""
  containerPorts:
    client: 2181
    election: 3888
    follower: 2888
    tls: 3181
  containerSecurityContext:
    enabled: true
    runAsNonRoot: true
    runAsUser: 1001
  customLivenessProbe: {}
  customReadinessProbe: {}
  customStartupProbe: {}
  dataLogDir: ""
  diagnosticMode:
    args:
    - infinity
    command:
    - sleep
    enabled: false
  enabled: true
  existingConfigmap: ""
  extraDeploy: []
  extraEnvVars: []
  extraEnvVarsCM: ""
  extraEnvVarsSecret: ""
  extraVolumeMounts: []
  extraVolumes: []
  fourlwCommandsWhitelist: srvr, mntr, ruok
  fullnameOverride: ""
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
  heapSize: 1024
  hostAliases: []
  image:
    debug: false
    digest: ""
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/zookeeper
    tag: 3.8.0-debian-11-r30
  initContainers: []
  initLimit: 10
  jvmFlags: ""
  kubeVersion: ""
  lifecycleHooks: {}
  listenOnAllIPs: false
  livenessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 30
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  logLevel: ERROR
  maxClientCnxns: 60
  maxSessionTimeout: 40000
  metrics:
    containerPort: 9141
    enabled: false
    prometheusRule:
      additionalLabels: {}
      enabled: false
      namespace: ""
      rules: []
    service:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
        prometheus.io/scrape: "true"
      port: 9141
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      honorLabels: false
      interval: ""
      jobLabel: ""
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scrapeTimeout: ""
      selector: {}
  minServerId: 1
  nameOverride: ""
  namespaceOverride: ""
  networkPolicy:
    allowExternal: true
    enabled: false
  nodeAffinityPreset:
    key: ""
    type: ""
    values: []
  nodeSelector: {}
  pdb:
    create: false
    maxUnavailable: 1
    minAvailable: ""
  persistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    dataLogDir:
      existingClaim: ""
      selector: {}
      size: 8Gi
    enabled: true
    existingClaim: ""
    selector: {}
    size: 8Gi
    storageClass: ""
  podAffinityPreset: ""
  podAnnotations: {}
  podAntiAffinityPreset: soft
  podLabels: {}
  podManagementPolicy: Parallel
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  preAllocSize: 65536
  priorityClassName: ""
  readinessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 5
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  replicaCount: 3
  resources:
    limits: {}
    requests:
      cpu: 250m
      memory: 256Mi
  schedulerName: ""
  service:
    annotations: {}
    clusterIP: ""
    disableBaseClientPort: false
    externalTrafficPolicy: Cluster
    extraPorts: []
    headless:
      annotations: {}
      publishNotReadyAddresses: true
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    nodePorts:
      client: ""
      tls: ""
    ports:
      client: 2181
      election: 3888
      follower: 2888
      tls: 3181
    sessionAffinity: None
    sessionAffinityConfig: {}
    type: ClusterIP
  serviceAccount:
    annotations: {}
    automountServiceAccountToken: true
    create: false
    name: ""
  sidecars: []
  snapCount: 100000
  startupProbe:
    enabled: false
    failureThreshold: 15
    initialDelaySeconds: 30
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  syncLimit: 5
  tickTime: 2000
  tls:
    client:
      auth: none
      autoGenerated: false
      enabled: false
      existingSecret: ""
      existingSecretKeystoreKey: ""
      existingSecretTruststoreKey: ""
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
      passwordsSecretKeystoreKey: ""
      passwordsSecretName: ""
      passwordsSecretTruststoreKey: ""
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
    quorum:
      auth: none
      autoGenerated: false
      enabled: false
      existingSecret: ""
      existingSecretKeystoreKey: ""
      existingSecretTruststoreKey: ""
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
      passwordsSecretKeystoreKey: ""
      passwordsSecretName: ""
      passwordsSecretTruststoreKey: ""
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
    resources:
      limits: {}
      requests: {}
  tolerations: []
  topologySpreadConstraints: []
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  volumePermissions:
    containerSecurityContext:
      runAsUser: 0
    enabled: false
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 11-debian-11-r27
    resources:
      limits: {}
      requests: {}
zookeeperChrootPath: ""
zookeeperConnectionTimeoutMs: 6000

HOOKS:
---
# Source: kafka/templates/kafka-provisioning.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: kafka-provisioning
  namespace: "kafka"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-18.4.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka-provisioning
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-18.4.0
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka-provisioning
      annotations:
    spec:
      
      securityContext:
        fsGroup: 1001
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      initContainers:
        - name: wait-for-available-kafka
          image: docker.io/bitnami/kafka:3.2.1-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -ec
            - |
              wait-for-port \
                --host=kafka \
                --state=inuse \
                --timeout=120 \
                9092;
              echo "Kafka is available";
          resources:
            limits: {}
            requests: {}
      containers:
        - name: kafka-provisioning
          image: docker.io/bitnami/kafka:3.2.1-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -ec
            - |
              echo "Configuring environment"
              . /opt/bitnami/scripts/libkafka.sh
              export CLIENT_CONF="${CLIENT_CONF:-/opt/bitnami/kafka/config/client.properties}"
              if [ ! -f "$CLIENT_CONF" ]; then
                touch $CLIENT_CONF

                kafka_common_conf_set "$CLIENT_CONF" security.protocol "PLAINTEXT"
              fi

              echo "Running pre-provisioning script if any given"
              
              

              kafka_provisioning_commands=(
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 2 \
                    --partitions 5 \
                    --config flush.messages=1 \
                    --config max.message.bytes=8.6e+07 \
                    --command-config ${CLIENT_CONF} \
                    --topic logs"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 2 \
                    --partitions 5 \
                    --config flush.messages=1 \
                    --config max.message.bytes=8.6e+07 \
                    --command-config ${CLIENT_CONF} \
                    --topic metrics"
              )

              echo "Starting provisioning"
              for ((index=0; index < ${#kafka_provisioning_commands[@]}; index+=1))
              do
                for j in $(seq ${index} $((${index}+1-1)))
                do
                    ${kafka_provisioning_commands[j]} & # Async command
                done
                wait  # Wait the end of the jobs
              done

              echo "Running post-provisioning script if any given"
              
              

              echo "Provisioning succeeded"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_SERVICE
              value: kafka:9092
          resources:
            limits: {}
            requests: {}
          volumeMounts:
      volumes:
MANIFEST:
---
# Source: kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kafka
  namespace: "kafka"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-18.4.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations:
automountServiceAccountToken: true
---
# Source: kafka/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-zookeeper-scripts
  namespace: kafka
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.1.1
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-scripts
  namespace: "kafka"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-18.4.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" "/bitnami/kafka/data/meta.properties" | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    # Configure zookeeper client

    exec /entrypoint.sh /run.sh
---
# Source: kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-zookeeper-headless
  namespace: kafka
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.1.1
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: zookeeper
---
# Source: kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-zookeeper
  namespace: kafka
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.1.1
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: zookeeper
---
# Source: kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
  namespace: "kafka"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-18.4.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: kafka
---
# Source: kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: "kafka"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-18.4.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: kafka
---
# Source: kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-zookeeper
  namespace: kafka
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.1.1
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/component: zookeeper
  serviceName: kafka-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-10.1.1
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: kafka
                    app.kubernetes.io/component: zookeeper
                namespaces:
                  - "kafka"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-11-r30
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: kafka-zookeeper-0.kafka-zookeeper-headless.kafka.svc.cluster.local:2888:3888::1 kafka-zookeeper-1.kafka-zookeeper-headless.kafka.svc.cluster.local:2888:3888::2 kafka-zookeeper-2.kafka-zookeeper-headless.kafka.svc.cluster.local:2888:3888::3 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: kafka-zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: "kafka"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-18.4.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/component: kafka
  serviceName: kafka-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-18.4.0
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
      annotations:
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/instance: kafka
                    app.kubernetes.io/component: kafka
                namespaces:
                  - "kafka"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: kafka
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.2.1-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: "kafka-zookeeper"
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: "INTERNAL://$(MY_POD_NAME).kafka-headless.kafka.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).kafka-headless.kafka.svc.cluster.local:9092"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_ZOOKEEPER_PROTOCOL
              value: PLAINTEXT
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "true"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVALS_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "2"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "5"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: kafka-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"

NOTES:
CHART NAME: kafka
CHART VERSION: 18.4.0
APP VERSION: 3.2.1

** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    kafka.kafka.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:

    kafka-0.kafka-headless.kafka.svc.cluster.local:9092
    kafka-1.kafka-headless.kafka.svc.cluster.local:9092
    kafka-2.kafka-headless.kafka.svc.cluster.local:9092

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.2.1-debian-11-r9 --namespace kafka --command -- sleep infinity
    kubectl exec --tty -i kafka-client --namespace kafka -- bash

    PRODUCER:
        kafka-console-producer.sh \
            
            --broker-list kafka-0.kafka-headless.kafka.svc.cluster.local:9092,kafka-1.kafka-headless.kafka.svc.cluster.local:9092,kafka-2.kafka-headless.kafka.svc.cluster.local:9092 \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            
            --bootstrap-server kafka.kafka.svc.cluster.local:9092 \
            --topic test \
            --from-beginning
